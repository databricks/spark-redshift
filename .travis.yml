language: scala
sudo: false
cache:
  directories:
    - $HOME/.ivy2
# There's no nicer way to specify this matrix; see
# https://github.com/travis-ci/travis-ci/issues/1519.
# Spark 1.5.0 only supports Java 7+.
matrix:
  include:
    # We only test Spark 1.4.1 with Hadooop 2.2.0 because
    # https://github.com/apache/spark/pull/6599 is not present in 1.4.1,
    # so the published Spark Maven artifacts will not work with Hadoop 1.x.
    - jdk: openjdk6
      scala: 2.10.5
      env: HADOOP_VERSION="2.2.0" SPARK_VERSION="1.4.1"
    - jdk: openjdk7
      scala: 2.10.5
      env: HADOOP_VERSION="1.0.4" SPARK_VERSION="1.5.0-rc3"
    - jdk: openjdk7
      scala: 2.10.5
      env: HADOOP_VERSION="1.2.1" SPARK_VERSION="1.5.0-rc3"
    - jdk: openjdk7
      scala: 2.10.5
      env: HADOOP_VERSION="2.2.0" SPARK_VERSION="1.5.0-rc3"
    # Scala 2.11 tests:
    - jdk: openjdk7
      scala: 2.11.7
      env: HADOOP_VERSION="2.2.0" SPARK_VERSION="1.5.0-rc3"
env:
  global:
    # AWS_REDSHIFT_JDBC_URL
    - secure: "RNkxdKcaKEYuJqxli8naazp42qO5/pgueIzs+J5rHwl39jcBvJMgW3DX8kT7duzdoBb/qrolj/ttbQ3l/30P45+djn0BEwcJMX7G/FGpZYD23yd03qeq7sOKPQl2Ni/OBttYHJMah5rI6aPmAysBZMQO7Wijdenb/RUiU2YcZp0="
    # AWS_REDSHIFT_PASSWORD
    - secure: "g5li3gLejD+/2BIqIm+qHiqBUvCc5l0qnftVaVlLtL7SffErp/twDiFP4gW8eqnFqi2GEC1c9Shf7Z9cOIUunNSBQZdYIVG0f38UfBeDP14nOoIuwZ974O5yggbgZhX0cKvJzINcENGoRNk0FzRwgOdCCiF05IMnRqQxI3C24fE="
    # AWS_REDSHIFT_USER
    - secure: "LIkY/ZpBXK3vSFsdpBSRXEsgfD2wDF52X8OZOlyBJOiZpS4y1/obj8b3VQABDPyPH95bGX/LOpM0vVM137rYgF0pskgVEzLMyZOPpwYqNGPf/d4BtQhBRc8f7+jmr6D4Hrox4jCl0cCKaeiTazun2+Y9E+zgCUDvQ8y9qGctR2k="
    # TEST_AWS_ACCESS_KEY_ID
    - secure: "kFmNLBHXPCIMdbDtHpULFzQtQqWJOO5Q3PtHyvfXE9oHaVwmQUfNd2ygoH5qnKuHO66bfPC7GSY5cMERNIFXsdhDEbRYFj6B4OgcQsYmKAG6b3+qcFYq6htJqkeFbOZB8kUN4YOjlkZg1lPXp/RV0Ac1g72mEsm2CvIoCXmDMYg="
    # TEST_AWS_SECRET_ACCESS_KEY
    - secure: "Z34uNm0HuDhyMG9jOhf0nmFR3RKxmF4o1NGg8mNdCuuIykelgY/YDi1LXf5fLz2yQXsmSwt308iq2E1n3Dv6ZBmS4a1s1dpZjTTfjXsyMpz0Hub9ia5B3Th9y0GPjgrMhazFoDf/OQnDxarthJEdJjvTeyo935F5kCK4gikzV8A="
    # STS_ROLE_ARN
    - secure: "cuyemI1bqPkWBD5B1FqIKDJb5g/SX5x8lrzkO0J/jkyGY0VLbHxrl5j/9PrKFuvraBK3HC56HEP1Zg+IMvh+uv0D+p5y14C97fAzE33uNgR2aVkamOo92zHvxvXe7zBtqc8rztWsJb1pgkrY7SdgSXgQc88ohey+XecDh4TahTY="
    # AWS_S3_SCRATCH_SPACE
    - secure: "LvndQIW6dHs6nyaMHtblGI/oL+s460lOezFs2BoD0Isenb/O/IM+nY5K9HepTXjJIcq8qvUYnojZX1FCrxxOXX2/+/Iihiq7GzJYdmdMC6hLg9bJYeAFk0dWYT88/AwadrJCBOa3ockRLhiO3dkai7Ki5+M1erfaFiAHHMpJxYQ="

script:
  - sbt -Dhadoop.testVersion=$HADOOP_VERSION -Dspark.testVersion=$SPARK_VERSION coverage test
  - if [ "$TRAVIS_SECURE_ENV_VARS" == "true" ]; then sbt -Dhadoop.version=$HADOOP_VERSION -Dspark.version=$SPARK_VERSION coverage it:test 2> /dev/null; fi
  - sbt scalastyle
  - sbt "test:scalastyle"
  - sbt "it:scalastyle"
after_success:
  - bash <(curl -s https://codecov.io/bash)
